{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. You **ONLY** change the parts of the code we asked you to, nowhere else (change only the coding parts saying `# YOUR CODE HERE`, nothing else);\n",
    "6. Don't add any new cells to this notebook;\n",
    "7. Fill in your group number and the full names of the members in the cell below;\n",
    "8. Make sure that you are not running an old version of IPython (we provide you with a cell that checks this, make sure you can run it without errors).\n",
    "\n",
    "Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you the following steps before submission for ensuring that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"23\"\n",
    "NAME1 = \"Michael Marne\"\n",
    "NAME2 = \"Anup V Padaki\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can run the following cell without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f19467ab4786e36e3793ec0a949a3872",
     "grade": false,
     "grade_id": "cell-5f64122350a250b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# HA2 - Recurrent Neural Networks\n",
    "\n",
    "Welcome to the second group home assignment!  \n",
    "The purpose of this assigment is to give you practical knowledge in how to implement recurrence in a neural network.  \n",
    "\n",
    "Every day you are exposed to sequences of data. For example text, video streams, audio, financial time series and medical sensors. Recurrence is therefore an important topic in the field of Machine Learning because it has the potential to solve real-world problems including the types of data above.  \n",
    "\n",
    "In this assignment, you will learn to:\n",
    " - Preprocess text data sets\n",
    " - Implement a vanilla RNN cell using only numpy\n",
    " - Text generation\n",
    " - Build a recurrent neural network with LSTM using Keras  \n",
    " - Neural machine translation\n",
    "\n",
    "**NOTE:** Task 1 (Shallow vanilla RNN) and Task 2 (Neural machine translation), are independent from each other. Task 2 asks you to train a NMT, which takes a while (specially without a GPU), so it might be efficient to start with task 2 and leave it running in the background while you solve task 1.\n",
    "\n",
    "Table of Contents:  \n",
    " [1 Shallow vanilla RNN](#1)  \n",
    "   [1.1 Preprocessing](#1.1)  \n",
    "     [1.1.1 Loading dataset](#1.1.1)  \n",
    "     [1.1.2 One-hot representations](#1.1.2)  \n",
    "   [1.2 RNN cell class](#1.2)  \n",
    "   [1.3 Training the RNN](#1.3)  \n",
    " [2 Neural machine translation](#2)  \n",
    "   [2.1 Pre-processing](#2.1)  \n",
    "     [2.1.1 Loading and inspecting dataset](#2.1.1)  \n",
    "     [2.1.2 Cleaning the dataset](#2.1.2)  \n",
    "     [2.1.3 Restricting sentence length and shuffling the data set](#2.1.3)  \n",
    "     [2.1.4 Word-to-index and index-to-word conversions](#2.1.4)  \n",
    "     [2.1.5 Padding](#2.1.5)  \n",
    "     [2.1.6 One-hot labels](#2.1.6)  \n",
    "   [2.2 Implementing a sequence-to-sequence model](#2.2)  \n",
    "     [2.2.1 Defining the architecture](#2.2.1)  \n",
    "     [2.2.2 Training the model](#2.2.2)  \n",
    "     [2.2.3 Evalutation](#2.2.3)  \n",
    "     [2.2.4 Testing](#2.2.4)  \n",
    "     \n",
    "**NOTE**: The tests available are not exhaustive, meaning that if you pass a test you have avoided the most common mistakes, but it is still not guaranteed that you solution is 100% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba23d62a14b6319db7d7e4056c2a3631",
     "grade": false,
     "grade_id": "cell-e3df9ba622ac9b73",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Shallow vanilla RNN <a class=\"anchor\" id=\"1\"></a>\n",
    "In the first part of this assignment, you will implement a recurrent neural network from scratch without using any framework. You will train this network to predict the next character of a text, which will result in a network that can generate new sentences.  \n",
    "\n",
    "Start by importing dependencies below  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afba6c87700a3a984c4e3dccfa61bfa1",
     "grade": false,
     "grade_id": "cell-625d522e7af1be75",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.tests.ha2Tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1e46aaed7aaad0c16768220a8439c71",
     "grade": false,
     "grade_id": "cell-161d4d51d8efd9a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Preprocessing <a class=\"anchor\" id=\"1.1\"></a>\n",
    "#### 1.1.1 Loading data set <a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "The text corpus to train your RNN on is going to be the book The metamorphosis by Franz Kafka. Run the cell below to load the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88ebbbf552d6a670317d098cce99d154",
     "grade": false,
     "grade_id": "cell-eddd3178e98f6819",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 118560 chars, 62 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('./utils/kafka.txt', 'r', encoding=\"utf-8\").read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f80e54d3658a982d5bd894a20d4d82f",
     "grade": false,
     "grade_id": "cell-7e993f61d8a98a13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1.1.2 One-hot representations <a class=\"anchor\" id=\"1.1.2\"></a>\n",
    "`data` is now a string, containing the contents of the book, with 80 unique characters. As usual, converting the labels (that is also the data in our case) into one-hot vectors is a good way for creating unbiased labels.  \n",
    "\n",
    "Below is a variable `one_hot_vectors` defined that maps every character in `chars` to a unique one-hot vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "609241821c3eb4ce1b929a9c727f355c",
     "grade": false,
     "grade_id": "cell-547e8c9d5874c36e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "one_hot_vectors = np.eye(len(chars))\n",
    "char_to_onehot = { val: one_hot_vectors[key,:].reshape(1,-1) for key, val in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa4da6c7dea9d40f5272f05f2c1d6f9f",
     "grade": false,
     "grade_id": "cell-05d8c9b256c98f8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 RNN cell class <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "Now you will implement the RNN class. For your convenience, this task is split into 4 subtasks, one for each method of the class. These methods will be later linked to a class definition (if you're curious, scroll down to 1.2.5).\n",
    "\n",
    "Instead of implementing a neural network of arbitrary length like in **IHA1**, you are only going to implement a single RNN cell. At each time-step, the RNN cell will have one input, $\\mathbf{x}_t$ , one hidden layer with a hidden state $\\mathbf{h}_t$, and one output $\\mathbf{y}_t$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ed0bfaac4000d97728b03afb10071a8",
     "grade": false,
     "grade_id": "cell-3a5034fe7f3b2df3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "![Simple RNN cell](utils/images/RNN-cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42e1be4c7ad74323a713e6cbc61a3533",
     "grade": false,
     "grade_id": "cell-719152523ee5b5b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The output at time-step $t$, referred to as $\\mathbf{y}_t$, is computed according to the following equations. Equation 1 and 2 together define the next hidden state $\\mathbf{h}_t$, and Equation 3 defines the unnormalized probability output $\\mathbf{z}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca5169401c992cb557a7e99f45e9405a",
     "grade": false,
     "grade_id": "cell-433ba61c3d3149cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "$$\n",
    "\\mathbf{z}_t =  \\mathbf{x}_t \\mathbf{W}_{xh} + \\mathbf{h}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h, \\tag{1}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{h}_t = \\tanh (\\mathbf{z}_t), \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_t =  \\mathbf{h}_t \\mathbf{W}_{hy} + \\mathbf{b}_y, \\tag{3}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_{h}$, and $\\mathbf{b}_{y}$ are the parameters of the RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6db71b1252e96bb33a732c034090ba57",
     "grade": false,
     "grade_id": "cell-651ce67679087d00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Calculating $\\mathbf{h}_{30}$ means that you need to need to have calculated every earlier hidden state $\\{\\mathbf{h}_t | t \\in \\{0, 1, ..., 29\\}\\}$. This can easier be understood by always imagining an unrolled RNN cell:\n",
    "![Simple RNN cell unrolled](utils/images/RNN-cell-unrolled.png)\n",
    "  \n",
    "\n",
    "Since it is a multiclass classification problem, the softmax activation function is going to be used to normalize the output of the RNN cell $\\mathbf{y}_t$, defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_t = softmax(\\mathbf{y}_t) = \\frac{e^{\\mathbf{y}_t - max(\\mathbf{y_t})}}{ \\sum^j e^{y_{j_t} - max(\\mathbf{y_t})}}. \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba386cb1102d60da82adaa9b0deec8af",
     "grade": false,
     "grade_id": "cell-d98b57a52debcfcf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.1 The initalization method\n",
    "\n",
    "The first step in creating the RNN class is to create its `__init__` method, where we create all the necessary attributes and initialize them. Complete the `init_routine` function below. Later this will be linked to the `__init__` method in the `SimpleRNN` class, so you're now developing the constructor of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0ab240474adc0f910903ac2a9321bb6b",
     "grade": true,
     "grade_id": "cell-c54cfa041a653943",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def init_routine(self, input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the RNN and define a cache with the starting value of the hidden state\n",
    "\n",
    "    Arguments:\n",
    "    self - an object of the class SimpleRNN\n",
    "    input_dim - an integer representing the number of inputs\n",
    "    hidden_dim - an integer representing the length of the hidden state vector\n",
    "    output_dim - an integer representing the number of outputs\n",
    "\n",
    "    Attributes:\n",
    "    input_dim - attribute initialized with the value of the argument `input_dim`\n",
    "    cache - a dictionary holding the values of `h_start`, `xs` and `hs` from the previous forward propagation call.\n",
    "            h_start - the value to initialize the hidden state with when `forward_prop is called. The value of\n",
    "                      `h_start` should be initialized to a `numpy.ndarray` vector of zeros \n",
    "            xs - the list of `xs` used in the last `forward_prop` call. Later used in `backward_prop`. Can be\n",
    "                 initialized to `None`\n",
    "            hs - the list of `hs` calculated in the last `forward_prop` call. Later used i `backward_prop`. Can\n",
    "                 be initialized to `None`\n",
    "    W_hh - weight matrix of shape (hidden_dim, hidden_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    W_xh - weight matrix of shape (input_dim, hidden_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    W_hy - weight matrix of shape (hidden_dim, output_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    b_h -  bias of shape (1, hidden_dim) and type `numpy.ndarray`. Initialized to all zeros\n",
    "    b_y -  bias of shape (1, output_dim) and type `numpy.ndarray`. Initialized to all zeros\n",
    "    \"\"\"\n",
    " \n",
    "    self.input_dim = input_dim\n",
    "    self.cache = {\n",
    "        'h_start': np.zeros((1, hidden_dim))\n",
    "    }\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    sigma = 0.01;\n",
    "    # mu = 0 (omitted)\n",
    "    \n",
    "    self.W_hh = sigma * np.random.randn(hidden_dim, hidden_dim);\n",
    "    self.W_xh = sigma * np.random.randn(input_dim, hidden_dim);\n",
    "    self.W_hy = sigma * np.random.randn(hidden_dim, output_dim);\n",
    "    self.b_h = np.zeros((1, hidden_dim));\n",
    "    self.b_y = np.zeros((1, output_dim));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f3fd0e811e9068cf8230649b148da21",
     "grade": false,
     "grade_id": "cell-e90af5372d951d74",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "00c3497a25df57a35a4d08cdb3406074",
     "grade": false,
     "grade_id": "cell-9595194787574666",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test init passed\n"
     ]
    }
   ],
   "source": [
    "test_init_routine(init_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e8b75e936deebb02f55f7ee19fdeb7f1",
     "grade": false,
     "grade_id": "cell-81027fe5c9d77a7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.2 Forward propagation\n",
    "\n",
    "Now that we have an initialization method for declaring objects of our class, we need a method for performing the forward propagation step. Complete the `forward_prop_routine` function. This will later be linked to the `forward_prop` routine of the `SimpleRNN` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0c3a5e91dfe32670754498a08282ad6f",
     "grade": true,
     "grade_id": "cell-76c90350a75c8c65",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop_routine(self, xs, reset_h=False):\n",
    "        \"\"\"\n",
    "        Performs forward propagation of the input `xs` in sequential order, and then returns all predictions `ys_pred`\n",
    "        The steps are: [Input x] -> [Hidden h] -> [Output unnormalized z] -> [Prediction probabilities y]\n",
    "\n",
    "        Arguments:\n",
    "        xs - a python list of one-hot characters to predict the next character from,\n",
    "             Has a shape of list([1,OUTPUT_DIM]) and length `len(xs)`\n",
    "        reset_h - boolean value, whether or not the hidden state should be reset\n",
    "\n",
    "        Returns:\n",
    "        ys_pred - a python list of prediction probabilities for each character in `xs`. There are `len(xs)`\n",
    "                  elements in ys_pred, each element `i` of shape (1, output_dim) and type `numpy.ndarray`\n",
    "                  that contains the probabilities of what the next character xs[i+1] can be\n",
    "\n",
    "        Example:\n",
    "        xs - [np.array([0,0,1]), np.array([0,1,0]), np.array([1,0,0])]\n",
    "        ys_pred (ground truth answer) - [np.array([0,1,0]), np.array([1,0,0]), np.array([...])]\n",
    "        \"\"\"\n",
    "        # initialize the list of predictions\n",
    "        ys_pred = [0] * len(xs)\n",
    "\n",
    "        # load the last hidden state of the previous batch\n",
    "        if reset_h:\n",
    "            self.cache['h_start'] = np.zeros((1, self.W_hh.shape[0]))\n",
    "        hs = {-1: self.cache['h_start']}\n",
    "\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        t = 0;\n",
    "        for x in xs:\n",
    "            z = np.array(x) @ self.W_xh + hs[t-1] @ self.W_hh + self.b_h;\n",
    "            h = np.tanh(z);\n",
    "            y = h @ self.W_hy + self.b_y;\n",
    "            \n",
    "            p = np.exp(y-y.max())/np.exp(y-y.max()).sum();\n",
    "            \n",
    "            \n",
    "            ys_pred[t] = p;     \n",
    "            hs[t] = h;\n",
    "            \n",
    "            t += 1;\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # save list of hidden state in cache to use later in backprop\n",
    "        self.cache = {\n",
    "            'hs': hs,\n",
    "            'xs': xs,\n",
    "            'h_start': hs[len(xs) - 1]\n",
    "        }\n",
    "\n",
    "        return ys_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffe0bd561a59bc4014d8dbf3508a899c",
     "grade": false,
     "grade_id": "cell-0cffc6927767f0fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved task 1.2.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef2fa074e9ad94c1c6d09dbb68d9530c",
     "grade": false,
     "grade_id": "cell-285af3fd03af809c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed, dimensions are correct\n"
     ]
    }
   ],
   "source": [
    "test_forward_prop_routine(init_routine, forward_prop_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b50d16ec08ef9da4541236762dd2283f",
     "grade": false,
     "grade_id": "cell-74f27955261e8417",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.3 Backward propagation\n",
    "\n",
    "Now that you developed methods for initialization and forward propagation, it's time to implement the backward propagation algorithm as a method for our class.\n",
    "\n",
    "The cross-entropy loss will be used for this task, and just like in **IHA1**, backward pass of the softmax activation function and the cross-entropy loss can be combined into a simple formula: $\\mathbf{\\hat{p_t}} - \\mathbf{p_t}$, where $\\mathbf{\\hat{p_t}}$ is the predicted output vector for time step $t$ and $\\mathbf{p_t}$ is the ground truth one-hot vector for time step $t$.\n",
    "The full collection of backward pass formulas that are required for implementing `backward_prop_routine` are shown in Equations 5-12:  \n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t}=\\hat{\\mathbf{p}}_t-\\mathbf{p}_t \\tag{5} ~,~ \\text{ for } t=1,\\dots,N, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{hy}}=\\sum_{t=1}^N\\mathbf{h}_t^T\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t}, \\tag{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{b}_{y}}=\\sum_{t=1}^N\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t},   \\tag{7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{h}_t}=\\frac{d \\mathcal{L}_t}{d \\mathbf{y}_t}\\mathbf{W}_{hy}^T+(1-\\mathbf{h}_{t+1}^2)\\odot\\frac{d\\mathcal{L}}{d\\mathbf{h}_{t+1}}\\mathbf{W}_{hh}^T   \\tag{8.1} ~,~ \\text{ for }t=1,\\dots,N-1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{h}_N}=\\frac{d \\mathcal{L}_t}{d \\mathbf{y}_N}\\mathbf{W}_{hy}^T,   \\tag{8.2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{z}_t}=(1-\\mathbf{h}_t^2)\\odot\\frac{d\\mathcal{L}}{d\\mathbf{h}_t}   ~,~ \\text{ for } t=1,\\dots,N,\\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{xh}}=\\sum_{t=1}^N\\mathbf{x}_t^T\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{hh}}=\\sum_{t=1}^N\\mathbf{h}_{t-1}^T\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{11}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{b}_h}=\\sum_{t=1}^N\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{12}\n",
    "$$\n",
    "where $\\odot$ is the <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\">hadamard product / elementwise multiplication</a>.  \n",
    "\n",
    "Complete the `backward_prop_routine` function below. This will later be linked to the `backward_prop` method of the `SimpleRNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c01e4f1d6658f6c2bf47727319ccef0",
     "grade": true,
     "grade_id": "cell-b0ed3463482a9b7a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backward_prop_routine(self, ys, ys_pred):\n",
    "    \"\"\"\n",
    "    Performs backward propagation, calculating the gradients of every trainable weight. The gradients should\n",
    "    be clipped into the interval [-5,5]\n",
    "\n",
    "    Arguments:\n",
    "    ys - a python list of true labels (one-hot vectors of type `numpy.ndarray` for every character to predict)\n",
    "         has a shape list( (1,OUTPUT_DIM) )\n",
    "    ys_pred - a python list of predicted labels (one-hot vectors of type `numpy.ndarray` for every character predicted)\n",
    "         has a shape list( (1,OUTPUT_DIM) )\n",
    "\n",
    "    Returns:\n",
    "    gradients - a dictionary of the gradients of every trainable weight. The keys are the weight variable names\n",
    "                and the values are the actual gradient value\n",
    "    \"\"\"\n",
    "\n",
    "    # extract from cache\n",
    "    hs = self.cache['hs']\n",
    "    xs = self.cache['xs']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    length = len(ys);\n",
    "    \n",
    "    dL_dy = np.zeros(length);\n",
    "    dL_dh = [0] * length;\n",
    "    dL_dz = [0] * length;\n",
    "    \n",
    "    # Preallocate memory for gradients\n",
    "    h_dim = self.W_hh.shape[0];\n",
    "    i_dim = self.input_dim;\n",
    "    o_dim = self.W_hy.shape[1];\n",
    "    dW_xh = np.zeros((i_dim, h_dim));\n",
    "    dW_hh = np.zeros((h_dim, h_dim));\n",
    "    dW_hy = np.zeros((h_dim, o_dim));\n",
    "    db_h  = np.zeros((1, h_dim));\n",
    "    db_y  = np.zeros((1, o_dim));\n",
    "    \n",
    "    # dL_dy = phat - p (eq 5)\n",
    "    dL_dy = np.array(ys_pred) - np.array(ys);\n",
    "    \n",
    "    # dL_dWhy = sum( h^T * dL_dy ) (eq 6) (here dL_dWhy = dW_hy)\n",
    "    # dL_dby = sum ( dL_dy ) (eq 7)\n",
    "    for t in range(length):\n",
    "        dW_hy += hs[t].transpose() @ dL_dy[t];\n",
    "        db_y  += dL_dy[t];\n",
    "        \n",
    "    \n",
    "    # dl_dh: Go backwards, start with dL_dh[N-1]: (eq 8.2)\n",
    "    W_hy_T = self.W_hy.transpose(); # save for repeated use\n",
    "    W_hh_T = self.W_hh.transpose();\n",
    "    \n",
    "    dL_dh[length-1] = dL_dy[length-1] @ W_hy_T; # last element\n",
    "    \n",
    "    for t in range(length-2,-1,-1):\n",
    "        # eq 8.1\n",
    "        dL_dh[t] = dL_dy[t] @ W_hy_T + (1 - np.square(hs[t+1])) * dL_dh[t+1] @ W_hh_T;\n",
    "        \n",
    "    \n",
    "    \n",
    "    # dL_dz = (1-h^2)*(dL_dh) (eq 9)\n",
    "    for t in range(length):\n",
    "        dL_dz[t] = (1 - np.square(hs[t])) * dL_dh[t];\n",
    "    \n",
    "    # dL_dWxh = sum(xT*dL_dz) (eq 10)\n",
    "    for t in range(length):\n",
    "        dW_xh += xs[t].transpose() @ dL_dz[t];\n",
    "    \n",
    "    \n",
    "    # dL_dWhh = sum (h[t-1]T * dL_dz) (eq 11)\n",
    "    # dL_dbh = sum(dL_dz) (eq 12)\n",
    "    for t in range(length):\n",
    "        dW_hh += hs[t-1].transpose() @ dL_dz[t];\n",
    "        db_h += dL_dz[t];\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Clip gradients to prevent explosion\n",
    "    for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    # save gradients into dict and return\n",
    "    gradients = {\n",
    "        'W_xh': dW_xh, \n",
    "        'W_hh': dW_hh, \n",
    "        'W_hy': dW_hy, \n",
    "        'b_h': db_h, \n",
    "        'b_y': db_y\n",
    "    }\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "78bd713cb2cf1e6db5d95f5d9e7d3f63",
     "grade": false,
     "grade_id": "cell-277e664111175140",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved tasks 1.2.1 and 1.2.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f82f89395a7f4650b44dadf40098f61",
     "grade": false,
     "grade_id": "cell-6c90c566fcf8388f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your BPTT implementation is correct.\n"
     ]
    }
   ],
   "source": [
    "test_backward_prop_routine(init_routine, forward_prop_routine, backward_prop_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7e173be209fcee69b1bb2b90b57efc1f",
     "grade": false,
     "grade_id": "cell-27bb37bddc1c8291",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.4 Applying the gradients\n",
    "\n",
    "Finally, we need a method that updates the paramters of the RNN cell, given the computed gradients of the loss with respect to each one of them. Complete the `apply_gradients_routine` function below. This will later be linked to the `apply_gradients` method of the `SimpleRNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "02925c28cb85c2167afded8f143c7dee",
     "grade": true,
     "grade_id": "cell-86233bb4464c563d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_gradients_routine(self, gradients, learning_rate):\n",
    "    \"\"\" Performs the weight update procedure. Updates every weight with its corresponding gradient \n",
    "    found in the `gradients` input dictionary\n",
    "\n",
    "    Arguments:\n",
    "    gradients - dictionary containing (key, value) pairs, where the key is a weight variable name of\n",
    "                the `SimpleRNN` class and the value is the gradient to apply to the matching weight\n",
    "                Example: {'W_xh':0.04, 'b_h':0.11}\n",
    "    learning_rate - the learning rate to use for this iteration of weight updates\n",
    "    \"\"\"\n",
    "\n",
    "    self.W_xh = apply_gradient(learning_rate, self.W_xh, gradients['W_xh']);\n",
    "    self.W_hh = apply_gradient(learning_rate, self.W_hh, gradients['W_hh']);\n",
    "    self.W_hy = apply_gradient(learning_rate, self.W_hy, gradients['W_hy']);\n",
    "    self.b_h = apply_gradient(learning_rate, self.b_h, gradients['b_h']);\n",
    "    self.b_y = apply_gradient(learning_rate, self.b_y, gradients['b_y']);\n",
    "    \n",
    "    \n",
    "def apply_gradient(rate, weight, grad):\n",
    "    # new_weight = old_weight - learning_rate * gradient\n",
    "    weight -= grad*rate;\n",
    "    return weight;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d89888760cc8351156dd4b38e2e37357",
     "grade": false,
     "grade_id": "cell-0660819488941eb7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved task 1.2.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5636499fd3046ff01566a81b29f477e2",
     "grade": false,
     "grade_id": "cell-bb2cdea3e67c6a3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test apply gradients passed\n"
     ]
    }
   ],
   "source": [
    "test_apply_gradients_routine(init_routine, apply_gradients_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1fef1cc499275c4ad36e86c6b06f11c6",
     "grade": false,
     "grade_id": "cell-30440dc54623f06c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.5 Putting everything together\n",
    "Now that all the parts of the RNN are implemented, we can define the class. The following cell defines the `SimpleRNN` class, linking all the functions you developed in the earlier tasks to corresponding methods. Additionally, it also implements another method, the `sample`, used to generate output from a trained RNN. Note that you don't have to change anything in this cell, just run it after solving the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5036021730af040e49d53fd5f45cb5d5",
     "grade": false,
     "grade_id": "cell-126a1305361a1287",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    The simple RNN class definition\n",
    "    Implements the initializations of the weights, the forward propagation from `x` to `y_pred`,\n",
    "    the backward propagation with respect to each trainable weight and the update weights rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    # __init__ method is now the initialization_routine function\n",
    "    __init__ = init_routine\n",
    "        \n",
    "    # forward_prop method is the forward_prop_routine function\n",
    "    forward_prop = forward_prop_routine\n",
    "    \n",
    "    # backward_prop method is the backward_prop_routine function\n",
    "    backward_prop = backward_prop_routine\n",
    "    \n",
    "    # apply_gradients method is the apply_gradients_routine function\n",
    "    apply_gradients = apply_gradients_routine\n",
    "\n",
    "\n",
    "    def sample(self, seed, n, char_to_onehot, chars):\n",
    "        \"\"\" Given a seed character `seed`, generates a sequence of `n` characters by continuously feeding the output\n",
    "        character of the RNN at time t as the input for the next time step at time t+1. If the RNN is trained well, \n",
    "        this method should be able to generate sentences that resembles some kind of structure. \n",
    "        The character should be generated with a probability of the outputs of the network!\n",
    "        \n",
    "        Arguments:\n",
    "        seed - a first character of type str to feed into the SimpleRNN\n",
    "        n - the length of the character sequence to generate\n",
    "        char_to_onehot - a dict that maps keys of characters to its one-hot representation. \n",
    "                         You created this dict earlier in the lab\n",
    "        char - a list of chars (vocabulary). You also created this list earlier in the lab\n",
    "        \"\"\"\n",
    "        \n",
    "        saved_cache = copy.deepcopy(self.cache)\n",
    "        \n",
    "        char_list = []\n",
    "        h = self.cache['h_start']\n",
    "\n",
    "        # seed one-hot char\n",
    "        x = char_to_onehot[seed]\n",
    "\n",
    "        for t in range(n):\n",
    "            \"\"\" not relevant anymore\n",
    "            # perform forward prop. `forward_prop` is not called because then the hidden state for the\n",
    "            # training procedure will be overwritten by the hidden state generates by this sampling procedure\n",
    "            h = np.tanh(np.dot(x, self.W_xh) + np.dot(h, self.W_hh) + self.b_h)\n",
    "            y = np.dot(h, self.W_hy) + self.b_y\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            \"\"\"\n",
    "            p = self.forward_prop(x)[0]\n",
    "            \n",
    "            # randomly pick a character with respect to the probabilities of the output of the network\n",
    "            ix = np.random.choice(range(self.input_dim), p=p.ravel())\n",
    "\n",
    "            x = np.zeros((1, self.input_dim))   \n",
    "            x[0,ix] = 1\n",
    "\n",
    "            # save the generated character\n",
    "            char_list.append(chars[ix])\n",
    "            \n",
    "        self.cache = saved_cache\n",
    "\n",
    "        return ''.join(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e2745ec97c2751a6491248946136dba",
     "grade": false,
     "grade_id": "cell-aaa08e666d9fed34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Training the RNN <a class=\"anchor\" id=\"1.3\"></a>\n",
    "In this section, code has been provided to you to train a `SimpleRNN` cell by predicting the next charcter in sequence.  \n",
    "\n",
    "The code below has the following properties: \n",
    " * Loop over `data`, using `seq_length` words at a time as a batch to train with\n",
    " * When `data` has been fully iterated through, start over again\n",
    " * Start over again with `data` in an infinite loop and then interrupt the running cell\n",
    " when results from `sample` are good enough.  \n",
    " * Perform `forward_prop`, `backward_prop` and  `apply_gradients` with each batch of characters\n",
    " * every 1000th iteration:\n",
    "     * compute the cross-entropy loss of the current batch\n",
    "     * sample a character sequence of length 200\n",
    "     * print the loss and the sampled sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss: 103.189794\n",
      "''fn(ed!wJleUSbB?:cE--dHo;)FTGlW MJjumep)\"!BL)Ie\"leO(YSxDdlWGn-PanpGtNwyQHYeY\"-OHç;GSv  :cC?qçs'YfpWtE.rçJtFPUNik?m-i\n",
      "fE';)EfSks:?sOhNeJçgGm.çliç\"\n",
      "aCus;taV'iDb(moMçoG.S'VQk(.J-(eB-\n",
      ":N:;imLy?!Içu.h\"EVE\n",
      "\n",
      "iteration 1000, loss: 78.652625\n",
      " mdeucfTctnleelVa tefanehese uifgreleinneUkttm lhra e Q yetu s  hiwi  ap eoiddnsatclkdytkskek attrthrge-tmenrridlrerreyhneadtovmrtmaijltgeele et ioirrshnoaaahnlhoi hsluucluce )lardhede i r mrleethecg \n",
      "\n",
      "iteration 2000, loss: 69.333051\n",
      " wot bhtswahoeoh sw eaollst y.t leao eiyheneoreu gichdoe ell gooeob kktl anAa twr ihee et a u dvkmhoee h.forp w yhaalnt e esbeotrtphuPUr ne oi e rh  asos ahr e mHmeooynmylht at  isr  sub aeonnmro  iel\n",
      "\n",
      "iteration 3000, loss: 67.649294\n",
      " abmhes fnat braesa fhiwi. be! whl,fun tort uovs osms yhmted hudkd wyeln miilore muroisb sehpig, teheosgn tl itipFuau de, telbocidhe lfhf .pk ;r whonrsnlrsy Thrfa o?,. poo oae; bos hisevneuhhs cht wis\n",
      "\n",
      "iteration 4000, loss: 68.628077\n",
      "aos, mter  fan g\n",
      "G. Fr hig ae titu hare hirsd hinme iIdt pocgil romune lireilg gonwdd ntei, wnard ket mhem tanh,e thred thon thactut nuthonfd tht;tey sas o wed wne horw sthimb alob .alheaiosavn araoh \n",
      "\n",
      "Epoch  1  completed. Total iterations done =  4742\n",
      "iteration 4742, loss: 64.897473\n",
      "nd nent we tat bar moes thue therm linm lo do wuin mas tlin tlen tlteG haag ins werllld 'wren uamnd hod STn tabw luawrem th ite fhes nslo Jinre wp onu sote snee ihg th wt wh. por thet sose s, khe, ton\n",
      "\n",
      "iteration 5742, loss: 66.276826\n",
      "wl angey coate gt dor stoia couthe n\n",
      "ave!eus ow, eom fnetinttesIHe toucdecg pI he phe thed he ceamy tharued deos beme. he fod. an tauthek wnect\"uclliH' ;as;d niet mihitse f to mn, soss bam, wOuurith m\n",
      "\n",
      "iteration 6742, loss: 56.322221\n",
      "t he wung womnecy simiku heselkge!ly ind his, FdingiaceiSn wmivy she har hat apd hac to the hed lame hek ind tou haackose st omer yereo- he ther roonfyer bod thurhqle ywem tooud shuuy semotwinl Ehaf b\n",
      "\n",
      "iteration 7742, loss: 64.400671\n",
      "te arwind docn iPlhe him dhem Ao utaut bid, bfshin he finidnd the tomcns qtos wb mondoangHhcl;og houv liel f atod moviw tis bias saast ato th wavlt bop Wout wisinduthe Guuthe Lgat as ln crho lhpedkgif\n",
      "\n",
      "iteration 8742, loss: 57.729363\n",
      "ny hingeruit ur sha co monup, he hiwt. jove, they bacar toth and huptha-, undd hiwidgubA bo thelers imegek ham hime thiçind ot wots tistoseve-letcuyr in tile cin moo Gpgoy eidp, oomg lf mis thin toue \n",
      "\n",
      "Epoch  2  completed. Total iterations done =  9484\n",
      "iteration 9484, loss: 58.352653\n",
      "nm sreek thecl angy srent\" Shed Gnd the werenel arrerker dle Se, cfein hil\"rering eaG she how suve svinte Hbe wlasll too wustoub, Jhe ticlsis ab an it ind Nine frimed the der the thnp, Ilm and nn cay \n",
      "\n",
      "iteration 10484, loss: 62.435699\n",
      "m the le wi arler onts hev'r er, Vtumego bad fid remt she shas wareor himr oud , wre tlind wed afy porvsn., tI wary s ours d the st nat\n",
      "d Yareps thet he werul\n",
      " anle he ha dron Gop llam, Uo in the gho \n",
      "\n",
      "iteration 11484, loss: 52.843562\n",
      "te tout nithe cuuws anwir. dxs nithev doit of mo him end wiwthad inhe oasken erenor yamr mon hl le he cithet brnthr ther Gfnyvs have outham Qlyo(Sel simisg nl? onsigt bens tomothel, ougk. meringan woo\n",
      "\n",
      "iteration 12484, loss: 64.225898\n",
      "lltinhen, mody wrenok oftisilleingent thel wass thas nul; mr whe mook cisouk he hid resitha nseredcevenlaser, fos couk bimere;or :ouln, saos be fat hocpint rory nno womldedeeye hinmendthid pryon, wo f\n",
      "\n",
      "iteration 13484, loss: 47.825102\n",
      "reth toyler ingp boken ware nlatj ond ff aed sofltnand ing anoun, hed bacingow th hend ats bathe pigane st aitlyy apled. Oponld, :ict of mace the brimker atterening him penurn ouss ato t an hose torit\n",
      "\n",
      "Epoch  3  completed. Total iterations done =  14226\n",
      "iteration 14226, loss: 54.631437\n",
      "ve d whe pow wict fhe the thot; Som leyit tos tw ard, Arles, bomtang hes arpeg; tle tyer tistt xump int bfof stewy \"re in thas ;or do we wom herk fing mist res mewate sert ous uist wstat hit crete nor\n",
      "\n",
      "iteration 15226, loss: 60.777241\n",
      "t s. be lv; That the stimed, tither ans un tlept ong mistr, butat tit, asserey erognt yacnady Iutt ch orey nas rad to ts anleven onid of woo liss. \"W ngingt anrs int withe heter routhe Iieg oren lo ss\n",
      "\n",
      "iteration 16226, loss: 51.107919\n",
      "k -qmust, bn. qofefrote tha motter qozry we tull biletry he hiof herceitherp hat hand hee thecer osed wutiched yting\" zoDe whitk at booc. Qef the putheW her culs boo heuss oreda He urd bnceve coulkem \n",
      "\n",
      "iteration 17226, loss: 65.004444\n",
      "d rome nomeitinn whrok doof anoiccansther;of bo has breiw ushsegors sisn hure hin tomhard thas rofpCy inte soll for, ind, anit thin tho beatthif fouln lusflita ghe tove ad shave dham any besterto hrow\n",
      "\n",
      "iteration 18226, loss: 38.964368\n",
      ". Gr aillyote thount and singish himg hid wadulithy\n",
      "Greand sreak do dad at hp haykey aisterigh as at enenpente shek werHay. no hit jassillemt allell theis be sous antuth, f mee rotthero tut erofargett\n",
      "\n",
      "Epoch  4  completed. Total iterations done =  18968\n",
      "iteration 18968, loss: 53.890875\n",
      "nd ang\n",
      "re; re tof ther Tulpfrict athed the G out of thead G onk hlercally, promr mode beds co che thetr't af he fle be precmrent Gregot whe thed un or, tored inged, anlighin ove fart to f toming fhole\n",
      "\n",
      "iteration 19968, loss: 60.102613\n",
      "s Gund is bot it hay hit thoued of çrexn they ta, if. Iboy the be wert Gre tho than; sade, lbee nound \"uve tsought, sim's ohe reamlto dacr cler; or anto Io thin nos as poomst at hes nedcisn toreghand \n",
      "\n",
      "iteration 20968, loss: 50.140669\n",
      "sen. Te herek snoy fouing n and le wast hos nt herm Grago chas womuse sisingor eout', e ptole toot ther uly and thoug havere s ano thin beef the qoutk lawe hasceelr, wo he ker hathen aule lakt outh ig\n",
      "\n",
      "iteration 21968, loss: 64.526214\n",
      "d amme, bed hiutin tabce bo listren his fro mcen the foreer stenet. \"habe wiwes both. Greatr, se hed. Gregor worct wapred aise. be Iusllyy fot abre fill. Grenos thingt, anf alint car outh a hning rten\n",
      "\n",
      "iteration 22968, loss: 33.941929\n",
      "tt hed oree, anes af thar bieg of had irto bed hed ookestrams., her wher in spe tist lighid soemtrolbenga'd at her andsing wtrunthe Btote end ance fad the inith Cace at wisist as atather wawe to beap \n",
      "\n",
      "Epoch  5  completed. Total iterations done =  23710\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(len(chars), 500, len(chars))\n",
    "\n",
    "# the number of characters to perform one update of the network with,\n",
    "# can be thought of as a batch\n",
    "seq_length = 25\n",
    "\n",
    "# the learning rate to use\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs_to_do = 5;\n",
    "num_epochs_done = 0;\n",
    "num_iterations_done = 0;\n",
    "\n",
    "while True: # the infinite loop\n",
    "    \n",
    "    # iterate over the data set\n",
    "    for i in range(len(data)//seq_length):\n",
    "        \n",
    "        # convert to one-hot\n",
    "        xs = [char_to_onehot[c] for c in data[i*seq_length:(i+1)*seq_length]]#inputs to the RNN\n",
    "        ys = [char_to_onehot[c] for c in data[i*seq_length+1:(i+1)*seq_length+1]]#the targets it should be outputting\n",
    "\n",
    "        # fp, bp and update\n",
    "        ys_pred = rnn.forward_prop(xs)\n",
    "        gradients = rnn.backward_prop(ys, ys_pred)\n",
    "        rnn.apply_gradients(gradients, learning_rate)\n",
    "\n",
    "        # print loss and sample a sentence every 1000th iteration\n",
    "        if i%1000==0:\n",
    "            loss = np.sum([ -1.0 *np.sum(y * np.log(y_pred + 1e-8)) for y, y_pred in zip(ys, ys_pred) ]) # x-entropy\n",
    "            print('iteration %d, loss: %f' % (num_iterations_done + i, loss))\n",
    "            text = rnn.sample('a', 200, char_to_onehot, chars)\n",
    "            print(text, end='\\n\\n')\n",
    "        \n",
    "    \n",
    "    num_epochs_done += 1;\n",
    "    num_iterations_done += len(data)//seq_length;\n",
    "    \n",
    "    print(\"Epoch \",num_epochs_done, \" completed. Total iterations done = \",num_iterations_done);\n",
    "    if (num_epochs_done >= num_epochs_to_do):\n",
    "        print(\"Training completed.\");\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8656704cd15109f98b427a7defe0fa1b",
     "grade": false,
     "grade_id": "cell-31b8664715bebff9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to produce sentences using the trained RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a947804c59cbf08c78563e7d7c9c51f",
     "grade": false,
     "grade_id": "cell-ed2dd488942f6809",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Sentence 0: ------\n",
      "in nen terptens wist. Doun hery hes Greenino cond d oot parus lhould tad thay thenct, the ha dof il in che cout intocs mat. )pery demte we thoy ega to toe the and dore trenta il. \"now uled at on anve \n",
      "------ Sentence 1: ------\n",
      "d too thas nof. ?orkean Me the ches firteotl er of toon, ouste. Dnceve titrobe noy plent'yint aid. Thing atl till and ald him he puly ist. Igor rathto chan the wh ne- fren lventry s lut the tieg tolen\n",
      "------ Sentence 2: ------\n",
      "d ing the staly tued, to te ren ove th or oud ned ore thauk cowet was bed his payaning; hl the the had soke tad. Gre thay sape here\" chepr gor ereint an; they der omtirel wow'nt t outcyter- qowy am al\n",
      "------ Sentence 3: ------\n",
      "r benep, nns omestofwing whe oublen llocr. Hett wat ch, and cloneny drest fit that th, steanle ding mot . Smotisich int tiblt ontg thim te sim bofs it the this afste ptheis wand raly rikand or thith t\n",
      "------ Sentence 4: ------\n",
      "d to te bite, coom ditt and tapse wing crlelenold yede corct on tor butple tod or;sbefur oud eele. Thed siess atller b anh lemaigitl qricgocr wat thay fed rake they Gremand is theacs and coutt ssmaly \n",
      "------ Sentence 5: ------\n",
      "d it ould th the Gro cal umd ypowr ho haik tores beanedte ile. Txint on is wasen thamer-utke, coo sor pestound yor tha lly fen to the urp timist on watllengould thin him to llowe. Pelveof chay thel of\n",
      "------ Sentence 6: ------\n",
      "m tern ale inite an le ker as -forke sutay, ans ;vome the wat the ded the d wa lpake hiat te the rilars, and thoug the the lecpal; snsigt ioul. Hsingibpe thomis plethe tut tor wed duti\"w \"xco thee sle\n",
      "------ Sentence 7: ------\n",
      "nd, an t h ur she staf if oulyrenkid oo the timted had hed theith oftad wishio wap. \"of te. Hs if bost on d ong woper tallute, of tit dely. bewo hve ter thit clud herderoDt stomread nor nithel bit the\n",
      "------ Sentence 8: ------\n",
      "d to the plenichty and remlen was of. Iom thaid trong mtellis ke. \"f the mod ot ous plereld watie sad eot the chist in uld \"on the whor her thing nin Aat il looghrrermeny enbe sulls co the thad wanly \n",
      "------ Sentence 9: ------\n",
      "d souts, then acmerely one warb wot ily ir sas ey he don wam th whsid wad s owe goutr torreed be whull, boob so renled bert henmo he carreed non te fle tad the the chime his the tad ior. ghay th woud \n"
     ]
    }
   ],
   "source": [
    "# kafka, including upper chars\n",
    "for i in range(10):\n",
    "    print('------ Sentence %i: ------' %i)\n",
    "    print(rnn.sample('a', 200, char_to_onehot, chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3dfdd352e2576a1aadc7df1de4c0ef03",
     "grade": false,
     "grade_id": "cell-3a78b102d91ed859",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Here are some generated sentences for reference how good the results should be. Not quite passing any spell checking program, but there are some real english words in there and the sentences seem to have a reasonable structure.\n",
    "\n",
    "    ------ Sentence 0: ------\n",
    "    ted eat aread noup, seed, but. ythed whithit tued that and in ask chemr tirme ble serpareing. he wionly memind. Dedteren drager, chat ther se stealeay. The kpay shir in wha dangereag he torearr ieve w\n",
    "    ------ Sentence 1: ------\n",
    "    swayly wase Mlamer io beell hom nofrreang, hr meed tould, Grere, werned coull gitr theQr coule verustengs at jeas whear', fasmer's, ad ho mofello goin \"hey in normablite chas a) he. Heenew \"thel seefr\n",
    "    ------ Sentence 2: ------\n",
    "    r's avethen harly dsundiigex; thinew, oug lertinestad to the wourd jhen sorywe, thever shene It, thour\" hawss He:thon's batiar's sfin, luike he. Ar uver. Af. qanilew ucheed attricgwaynong roked t- an \n",
    "    ------ Sentence 3: ------\n",
    "    tor\n",
    "     unde, thab haw dtidtly kad sendeder, ithed far theme, veedlly nowlyin; and )rece thete disse, out louthe, row the rapeasterse at. Heed rnweem. \"ut he bleitny Grerorot ie reryibe. So in., Hers ane\n",
    "    ------ Sentence 4: ------\n",
    "    ding, fitse. . Whisly andit, she haing \"ya bistatily dlatkedor whing saded theust, but the stere stotady is eleaprcentake, got to mame luch ally, mus llablly, ach caling, at what, he want aar athen d'\n",
    "    ------ Sentence 5: ------\n",
    "    cll\"end sat what herpingoth thouthar ald beinvaver.\n",
    "\n",
    "    Tere sreaned on in she was athar thatssu fres is at lees morther all sroaven to somichary mothtrey norytiot a tior sow bercead lassedore towist at \n",
    "    ------ Sentence 6: ------\n",
    "    gly.\n",
    "\n",
    "    Shen in shend comrie t oreth ubserwer he dist in commicherr. Wrmeathithemy nougracl.y'e sate horkllomisted betrnowhrous wfverthr bleatekeM tusinitedy Herach mop wislrocly theirse\n",
    "     on illy ans in\n",
    "    ------ Sentence 7: ------\n",
    "    dd, puting ot been ay theed . Hemula, he farer\". The h hanor, and dandid to the sedertereveng untint.\n",
    "\n",
    "    Wken\", surme ceared thet lomp io wotcouUir onsur\"ont ar with, thancMmperny him., is sas bees coul\n",
    "    ------ Sentence 8: ------\n",
    "    d natmen\", he theretotist wist, Gregely, arly re caule he praser, and woll buccem, then atse. Hemu haded ditince and dleyrist comply. \"Vm inen beglings wet ou't dot, plest willay theren lipsest inpren\n",
    "    ------ Sentence 9: ------\n",
    "    byed Gregst oonsbecemed thet ad herladble, din ther, and at. Numferringtice st ead thelr way mors. Ium loomul themo head roon inf thet the. \"lencorn was ned in tuc'e foot butimed, -feritn be thand Yun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5e457fb0c08a07d9504ba0290f2b79e",
     "grade": false,
     "grade_id": "cell-e01fb706e3e77e40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You may try different learning rates, different number of neurons for the hidden state and try to prune the text from special characters and upper characters, but the fact is that this is only one single RNN cell and it is as good as it gets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7be4351414f3dc4e27233b3870e8704",
     "grade": false,
     "grade_id": "cell-69f0dab9e82b0115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Neural Machine Translation <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "In this task you will implement a small neural machine translator using the Keras API. The final model will be able to translate english sentences into spanish sentences. Unlike the previous task, the input and output at each time step will now represent an entire word instead of a character.  \n",
    "\n",
    "The task is divided into two main sections:\n",
    " * Pre processing of data\n",
    " * Building a sequence-to-sequence RNN\n",
    " \n",
    "The principle of sequence-to-sequence modelling is shown in the figure below\n",
    "![seq2seq unrolled](utils/images/seq2seq.png)\n",
    "In your case, an english sentence is input to the encoder network, one word at each time step. After the sentence has been fed, there will now be a representation of the entire sentence encoded into the hidden state of the RNN. The hidden state of the decoder network is thereafter initialized with this representation and tries to generate the sequence of spanish words from the initial hidden state.  \n",
    "\n",
    "Run the cell below to import the necessary packages to get started with the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "effc1f6c16ac094eaf6f8ab899102549",
     "grade": false,
     "grade_id": "cell-245a4886ba97957a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils.tests.ha2Tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aac6ec95a53779286e1b12d2b17c369",
     "grade": false,
     "grade_id": "cell-b4d9ff24542835da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Pre-processing <a class=\"anchor\" id=\"2.1\"></a>\n",
    "The dataset to use consists of bilingual sentence pairs. Each sentence in the list of english sentences has a corresponding spanish translation at the same index in the list of spanish sentences.  \n",
    "\n",
    "Basic preprocessing has already been performed such as:  \n",
    " * to lower case\n",
    " * remove any padded spaces at the start and end of the row (trim)\n",
    " * convert from unicode character set to the ASCII character set. Example: á --> a and ñ --> n.\n",
    " * If the character is a question mark, punctuation or exclamation mark: ? ! ., put a space to the left of the character to make the char into a separate word.\n",
    " * remove any non-letter character\n",
    " * split each line into a tuple, where the left element is the source language sentence and the right element is the target language sentence\n",
    " * remove any sentence pair with number of words more than 5\n",
    "\n",
    "#### 2.1.1 Loading and inspecting the data set <a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "Lets first load the english-spanish sentence pairs into memory and inspect some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecb81ee7fcde8806c59c1a7ac4079a08",
     "grade": false,
     "grade_id": "cell-3082498e5d14fae7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 114282 sentence pairs\n",
      "The shortest english sentence is 2 words\n",
      "The shortest spanish sentence is 2 words\n",
      "The longest english sentence is 5 words\n",
      "The longest spanish sentence is 5 words\n",
      "\n",
      "10 Random short sentence pairs:\n",
      "it wasnt hard . \t no era dificil .\n",
      "its hot in here . \t hace calor aqui .\n",
      "you missed . \t fallaste .\n",
      "theyre old friends . \t ellos son viejos amigos .\n",
      "somebody messed up . \t alguien la ha liado .\n",
      "i must go there . \t debo ir alla .\n",
      "its so beautiful . \t es tan bello .\n",
      "youre doing it right . \t lo estas haciendo bien .\n",
      "call tom immediately . \t llama a tom inmediatamente .\n",
      "you are overworked . \t tienes exceso de trabajo .\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "lines = open('utils/spa.txt', 'r', encoding='UTF-8').read().strip().split('\\n')\n",
    "data = pickle.load( open( \"utils/spa-preprocessed.pkl\", \"rb\" ) )\n",
    "spa, eng = data['spa'], data['eng']\n",
    "SEQ_MAX_LEN = 5\n",
    "\n",
    "print('There are %i sentence pairs' % len(lines))\n",
    "print('The shortest english sentence is %i words' \n",
    "      % np.min(list(map(lambda line: len(line.split(' ')), eng))))\n",
    "print('The shortest spanish sentence is %i words' \n",
    "      % np.min(list(map(lambda line: len(line.split(' ')), spa))))\n",
    "print('The longest english sentence is %i words' \n",
    "      % np.max(list(map(lambda line: len(line.split(' ')), eng))))\n",
    "print('The longest spanish sentence is %i words' \n",
    "      % np.max(list(map(lambda line: len(line.split(' ')), spa))))\n",
    "print('\\n10 Random short sentence pairs:')\n",
    "for i in range(10):\n",
    "    ix = np.random.choice(int(len(eng)))\n",
    "    print(eng[ix],'\\t',spa[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e9b6db781a32688bf0e80b7c3fed9c3",
     "grade": false,
     "grade_id": "cell-1d324eeeaec95b0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.4 Word-to-index and index-to-word conversions <a class=\"anchor\" id=\"2.1.4\"></a>\n",
    "Just like in Task 1 earlier, you need dictionaries to transform between (in this case) the words and unique number representations. The following dictionaries/lists are implemented:  \n",
    "\n",
    " * `eng_ix_to_word` - a python list where each index is a unique number that represents the english word stored as value.\n",
    "                      `eng_ix_to_word[0]` should equal to `ZERO`\n",
    "                      `enx_ix_to_word[-1]` (the last element) should equal to `UNK` (unknown)\n",
    " * `eng_word_to_ix` - like `eng_ix_to_word` but reversed, a dictionary mapping every word into the unique number. The key is a word that also exists in `eng_ix_to_word` and the value is an integer, that matches the index of the word in `eng_ix_to_word`\n",
    " * `spa_ix_to_word` - same principle as `eng_ix_to_word`, but from the vocabulary of `spa`\n",
    "                      `spa_ix_to_word[0]` should equal to `ZERO`\n",
    "                      `spa_ix_to_word[-1]` (the last element) should equal to `UNK` (unknown)\n",
    " * `spa_word_to_ix` - same principle as `eng_word_to_ix`, but from the vocabulary of `spa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0a5cc008e4b29824644f5553837a564",
     "grade": false,
     "grade_id": "cell-a459812ea1439340",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "eng_vocab = set([word for sentence in eng for word in sentence.split(' ')])\n",
    "eng_ix_to_word = ['ZERO'] + list(eng_vocab) + ['UNK']\n",
    "eng_word_to_ix =  { word: index for index, word in enumerate(eng_ix_to_word)}\n",
    "spa_vocab = set([word for sentence in spa for word in sentence.split(' ')])\n",
    "spa_ix_to_word = ['ZERO'] + list(spa_vocab) + ['UNK']\n",
    "spa_word_to_ix =  { word: index for index, word in enumerate(spa_ix_to_word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57904094aa594914b948bcb405a6e14b",
     "grade": false,
     "grade_id": "cell-5cfb758da1a8770a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we convert the words of `eng` and `spa` into their corresponding index numbers  \n",
    "\n",
    "Run the cell below to initialize these variables:  \n",
    " * X - A list of sentences. Each sentence is represented by a list where the i'th element is the index number representing the i'th word in the sentence string.\n",
    " * Y - same principle as `X`, but contains the converted spanish sentences  \n",
    " \n",
    " Example:\n",
    " if `eng_word_to_ix` is defined as `{'good':0, 'morning':1}`  \n",
    " and `eng` is defined as `[\"good morning\", \"morning\"]`  \n",
    " then `X` should result in `[[0,1],[1]]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a63e9c0f34e4859e94fc06460e1ae328",
     "grade": false,
     "grade_id": "cell-41597a7b019841d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = [ [eng_word_to_ix[word] for word in sentence.split(' ')] for sentence in eng]\n",
    "Y = [ [spa_word_to_ix[word] for word in sentence.split(' ')] for sentence in spa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f8a26208e3cccbe6d665d6fc45284ca",
     "grade": false,
     "grade_id": "cell-e87e7a013d284386",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.5 Padding <a class=\"anchor\" id=\"2.1.5\"></a>\n",
    "Since the sentences are of variable lengths, padding is needed to make every sentence equal length. Every sentence list in `X` and `Y` are padded with zeros to make every list equal to the length of the maximum sentence length. The zeros are **prepended** before the first word.  \n",
    "\n",
    "Example:  \n",
    "if `X` is defined as `[[1,2,3], [1,2], [1]]`   \n",
    "then after applying padding, X is equal to `[[1,2,3], [0,1,2], [0,0,1]]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59c6a6079e6a56f0097e1a25a392841a",
     "grade": false,
     "grade_id": "cell-ff60390bb7636a51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=SEQ_MAX_LEN, dtype='int32')\n",
    "Y = pad_sequences(Y, maxlen=SEQ_MAX_LEN, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0635f1fe84873fa1e26026b667fde9f",
     "grade": false,
     "grade_id": "cell-502981a32a6c5b6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.6 One-hot labels <a class=\"anchor\" id=\"2.1.6\"></a>\n",
    "The last pre processing converting the data into one-hot vectors.  \n",
    "\n",
    "For this task, **only** the labels (spanish words) are going to be transformed into one-hot vectors. The data points of english words are only going to be transformed into their corresponding numeric values by `eng_word_to_ix` because `Embedding` from the keras API already implements converting the inputs to one-hot vectors.  \n",
    "\n",
    "Transform `Y` to have the following properties:\n",
    " * define `Y` as a numpy matrix of shape (number of sentences, number of words of longest sentence, one-hot vector length). \n",
    " * dimension 0 represents each sentence (batch).\n",
    " * dimension 1 represents each word in a sentece (padded with zeros for equal length)\n",
    " * dimension 2 represents the one-hot vector for that particular word in that particular sentence. To pass the test, a onehot representation of word `1023` should be a zero vector with its 1023th index set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "578991fcbd3cff75d1b64387f0349fec",
     "grade": true,
     "grade_id": "cell-1afbd922885825dd",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip: the shape of Y should be: (22947, 5, 8805)\n"
     ]
    }
   ],
   "source": [
    "# TODO: convert each index number in Y into its corresponding one-hot vector\n",
    "num_output = len(spa_word_to_ix)\n",
    "print(\"Tip: the shape of Y should be: (%i, %i, %i)\" % (len(Y), len(Y[0]), num_output))\n",
    "\n",
    "from keras.utils import to_categorical;\n",
    "\n",
    "def Y_to_onehot(Y, num_output):\n",
    "    # Complete function according to description above\n",
    "    \n",
    "    Y_onehot = np.zeros((len(Y),len(Y[0]),num_output));   \n",
    "    for sentence in range(len(Y)):\n",
    "        for word in range(len(Y[sentence])):       \n",
    "            Y_onehot[sentence,word,Y[sentence,word]] = 1;\n",
    "    \n",
    "    return Y_onehot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d7554a570e89bddeb80b6b5d3bcee68",
     "grade": false,
     "grade_id": "cell-4b7e350b4705b7fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "403577ab8a2dd3aad46eca2ec0499148",
     "grade": false,
     "grade_id": "cell-4ca82916bbb49405",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "# test case \n",
    "test_Y_to_onehot(Y_to_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da368670f710fd04e2151601f977e521",
     "grade": false,
     "grade_id": "cell-ce0b6f9695d8e625",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "When you passed the test, run the cell below to convert the sentences of Y into lists of onehot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a66db52162f451acbc5f7c62afbc18aa",
     "grade": false,
     "grade_id": "cell-fd9b28bb297d5dde",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Y_onehot = Y_to_onehot(Y, num_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "58a8b6ccd149f8ae93f59795efd4176d",
     "grade": false,
     "grade_id": "cell-c5a0ba0c96903a79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Implementing a sequence-to-sequence model <a class=\"anchor\" id=\"2.2\"></a>\n",
    "In this section you will define, train and evaluate a sequence-to-sequence RNN architecture with the help of the Keras API.  \n",
    "\n",
    "#### 2.2.1 Defining the architecture <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "\n",
    "Now you are going to define the architecture for the translator. There are several possible architectures for performing translation, which is a sequence-to-sequence task (an input sequence, the English sentence, mapped to an output sequence, the Spanish sentence). If you take inspiration from searching the internet, you might come across neural machine translators such as [Google's Neural Machine Translation System](http://tsong.me/blog/google-nmt/). This kind of deep architecture was trained for a week using 100 GPUs, which is completely out of reach for the purposes of this assignment. Instead we will ask you to implement a simpler architecture that, even though it doesn't possess all the important parts of a translation network, still performs acceptably.\n",
    "\n",
    "The architecture you are required to implement will closely resemble the image shown at the start of this task. First the input sequence is fed to an encoder network, which is in charge of summarizing the entire sequence in its internal memory. This internal memory is then fed to a decoder network, which uses this summarized content to produce the output sequence. Both the encoder and the decoder networks will be comprised of a single LSTM unit, with a memory cell of arbitrary dimension. Since we won't stack LSTM layers, this can be seen as a \"toy\" version of a complete LSTM architecure.\n",
    "\n",
    "#### Practical remarks\n",
    "\n",
    "Note that the input sequence for each training sentence is a vector with 5 elements, where each element corresponds to the number of that particular word in the vocabulary. \n",
    "\n",
    "Because similar word indexes don't correspond to similar words (e.g. index 133 corresponds to 'joke', whereas index 134 corresponds to 'silence'), it's easier to train our network if we map this data to a space where the semantic meaning of the word is related to its geometric position. For instance, we would like the word 'car' to be closer to the word 'bus' than it is to 'tree'. This can be accomplished with the `Embedding` layer module from Keras, which tries to find a suitable representation for positive integers in a higher-dimensional space by using dimensionality reduction techniques in the matrix of co-ocurrence statistics of our input data. This layer can use an already trained mapping (like Word2Vec), but in this exercise we'll train it ourselves.\n",
    "\n",
    "Besides deciding how to encode the input, there is another important practical remark to be done. Since the decoder network will also be implemented as an LSTM, it will require inputs. According to what was described earlier, the only important data for this network is the memory cell at each time-step. Unfortunately, there is no simple way to create an LSTM cell without inputs (you can, of course, customize the LSTM layer to not receive any inputs), so we'll simply feed zeros as input at each time-step and it's likely that the optimization procedure will tune the decoder weights to disregard this input eventually (because it's completely uncorrelated to the input and the label, so it doesn't help with the prediction at all).\n",
    "\n",
    "#### Keras tips\n",
    "\n",
    "- It's necessary to use the [Model class API](https://keras.io/models/model/) for implementing this model (because it has two inputs, the input sequence and the zeros to the decoder). Be sure to familiarize yourself with this way of defining models in Keras.\n",
    "- You can define an LSTM cell using the [LSTM layer](https://keras.io/layers/recurrent/#lstm).\n",
    "- The LSTM layer by default outputs only the output for the last time-step, which is great for the encoder part, since we don't care about its outputs anyway. However, for the decoder network we want the outputs at all time steps. To accomplish this, you can set the argument `return_sequences` to `True` when creating the LSTM layer.\n",
    "- You will need to feed the memory cell content of the encoder network at the last time-step to the decoder network. You can ask the LSTM layer to also output its memory cell contents by setting the `return_state` argument to `True` when creating the layer (this makes the LSTM layer output three things: the actual LSTM output and two variables with different parts of the LSTM internal memory cell).\n",
    "- You can specify the initial state of LSTM layers symbolically by calling them with the keyword argument  `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the LSTM layer.\n",
    "\n",
    "\n",
    "Now, implement your sequence-to-sequence architecture in the function `create_model` by following the descriptions of the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d52b0195834f0be18715584290ff3e7",
     "grade": true,
     "grade_id": "cell-38f1aa584c956f28",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Add your import statements here\n",
    "from keras import Input, Model\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import regularizers\n",
    "\n",
    "def create_model(input_n, X_seq_len, output_n, Y_seq_len, hidden_dim, embedding_dim):\n",
    "    \"\"\" Define a keras sequence-to-sequence model. \n",
    "    \n",
    "    Arguments:\n",
    "    input_n - integer, the number of inputs for the network (the length of a one-hot vector from `X`)\n",
    "    X_seq_len - integer, the length of a sequence from `X`. Should be constant and you made sure by using padding\n",
    "    output_n - integer, the number of outputs for the network (the length of a one-hot vector from `Y`)\n",
    "    Y_seq_len - integer, the length of a sequence from `Y`. Should be constant and you made sure by using padding\n",
    "    hidden_dim - integer, number of units in the LSTM's memory cell.\n",
    "    embedding_dim - output dimension of the embedding layer.\n",
    "    \n",
    "    Returns:\n",
    "    The compiled keras model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input and embedding layers\n",
    "    encoder_input_layer = Input(shape=[X_seq_len])\n",
    "    encoder_embedding_layer = Embedding(input_n, embedding_dim, input_length=X_seq_len, mask_zero=True)(encoder_input_layer)\n",
    "    \n",
    "    # Create the encoder LSTM.\n",
    "    # YOUR CODE HERE\n",
    "    encoder_lstm_layer = LSTM(\n",
    "        hidden_dim,\n",
    "        return_state=True,\n",
    "        return_sequences=True#,\n",
    "        #kernel_regularizer=regularizers.l2(0.01)\n",
    "    );\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm_layer(encoder_embedding_layer);\n",
    "    encoder_state = [state_h, state_c];\n",
    "    \n",
    "    # Null input (should be fed with zeros) and repeating it the same number of times as there are words in the \n",
    "    # target sentence\n",
    "    null_input = Input(shape=[1])\n",
    "    repeated_null = RepeatVector(Y_seq_len)(null_input)\n",
    "    \n",
    "    # Create the decoder LSTM (feed it with the memory of the encoder network). Call it `decoder_layer`\n",
    "    # YOUR CODE HERE\n",
    "    decoder_lstm = LSTM(\n",
    "        hidden_dim,\n",
    "        return_state=True,\n",
    "        return_sequences = True#,\n",
    "        #kernel_regularizer=regularizers.l2(0.01)\n",
    "    );\n",
    "    decoder_layer, _, _ = decoder_lstm(repeated_null, initial_state=encoder_state);\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add a fully connected layer and a softmax to the outputs of the decoder\n",
    "    decoder_fully_connected = TimeDistributed(Dense(output_n))(decoder_layer)\n",
    "    decoder_softmax = Activation('softmax')(decoder_fully_connected)\n",
    "    \n",
    "    # Create final model and compile it\n",
    "    model = Model([encoder_input_layer, null_input], decoder_softmax)\n",
    "    \n",
    "    # Compile the model. Use a loss function, optimizer, and metrics of your choice\n",
    "    # YOUR CODE HERE\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']);\n",
    "    \n",
    "    # Add these arguments to the model for convenience\n",
    "    model.hidden_dim = hidden_dim\n",
    "    model.embedding_dim = embedding_dim\n",
    "    \n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7069ad2bd986c2fa40a6df328637a021",
     "grade": false,
     "grade_id": "cell-330f1845a4caf93a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.2 Training the model <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "Now create the model and train it. Try out different hyper-parameters if you're not satisfied with the result. For obtaining a good speedup using the GPU, opt for large number of memory cells and large batch sizes (although not too large, that also has drawbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d27df11c0d2a35548069e5b2adb14f5",
     "grade": true,
     "grade_id": "cell-8784b37d2b07cbfc",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got new file path:  logs\\LSTM20\n",
      "Training...\n",
      "Train on 18357 samples, validate on 4590 samples\n",
      "Epoch 1/50\n",
      " - 18s - loss: 6.0750 - acc: 0.3114 - val_loss: 5.1433 - val_acc: 0.3282\n",
      "Epoch 2/50\n",
      " - 14s - loss: 4.9488 - acc: 0.3239 - val_loss: 4.9397 - val_acc: 0.3326\n",
      "Epoch 3/50\n",
      " - 14s - loss: 4.7394 - acc: 0.3263 - val_loss: 4.7989 - val_acc: 0.3365\n",
      "Epoch 4/50\n",
      " - 16s - loss: 4.5862 - acc: 0.3386 - val_loss: 4.6874 - val_acc: 0.3489\n",
      "Epoch 5/50\n",
      " - 17s - loss: 4.4442 - acc: 0.3462 - val_loss: 4.5771 - val_acc: 0.3634\n",
      "Epoch 6/50\n",
      " - 17s - loss: 4.2818 - acc: 0.3790 - val_loss: 4.4329 - val_acc: 0.4010\n",
      "Epoch 7/50\n",
      " - 16s - loss: 4.1026 - acc: 0.4074 - val_loss: 4.3106 - val_acc: 0.4138\n",
      "Epoch 8/50\n",
      " - 17s - loss: 3.9444 - acc: 0.4227 - val_loss: 4.1999 - val_acc: 0.4235\n",
      "Epoch 9/50\n",
      " - 17s - loss: 3.7978 - acc: 0.4395 - val_loss: 4.0995 - val_acc: 0.4363\n",
      "Epoch 10/50\n",
      " - 16s - loss: 3.6582 - acc: 0.4545 - val_loss: 4.0101 - val_acc: 0.4451\n",
      "Epoch 11/50\n",
      " - 16s - loss: 3.5222 - acc: 0.4685 - val_loss: 3.9207 - val_acc: 0.4548\n",
      "Epoch 12/50\n",
      " - 17s - loss: 3.3897 - acc: 0.4816 - val_loss: 3.8420 - val_acc: 0.4648\n",
      "Epoch 13/50\n",
      " - 17s - loss: 3.2619 - acc: 0.4946 - val_loss: 3.7818 - val_acc: 0.4701\n",
      "Epoch 14/50\n",
      " - 17s - loss: 3.1408 - acc: 0.5061 - val_loss: 3.6915 - val_acc: 0.4803\n",
      "Epoch 15/50\n",
      " - 17s - loss: 3.0297 - acc: 0.5161 - val_loss: 3.6109 - val_acc: 0.4885\n",
      "Epoch 16/50\n",
      " - 17s - loss: 2.9187 - acc: 0.5263 - val_loss: 3.5510 - val_acc: 0.4940\n",
      "Epoch 17/50\n",
      " - 17s - loss: 2.8032 - acc: 0.5373 - val_loss: 3.4787 - val_acc: 0.5003\n",
      "Epoch 18/50\n",
      " - 17s - loss: 2.6899 - acc: 0.5483 - val_loss: 3.4194 - val_acc: 0.5056\n",
      "Epoch 19/50\n",
      " - 18s - loss: 2.5797 - acc: 0.5586 - val_loss: 3.3608 - val_acc: 0.5119\n",
      "Epoch 20/50\n",
      " - 18s - loss: 2.4738 - acc: 0.5687 - val_loss: 3.3063 - val_acc: 0.5173\n",
      "Epoch 21/50\n",
      " - 18s - loss: 2.3674 - acc: 0.5784 - val_loss: 3.2488 - val_acc: 0.5226\n",
      "Epoch 22/50\n",
      " - 18s - loss: 2.2621 - acc: 0.5885 - val_loss: 3.1901 - val_acc: 0.5291\n",
      "Epoch 23/50\n",
      " - 17s - loss: 2.1608 - acc: 0.5986 - val_loss: 3.1423 - val_acc: 0.5329\n",
      "Epoch 24/50\n",
      " - 16s - loss: 2.0597 - acc: 0.6097 - val_loss: 3.0858 - val_acc: 0.5429\n",
      "Epoch 25/50\n",
      " - 14s - loss: 1.9561 - acc: 0.6226 - val_loss: 3.0448 - val_acc: 0.5479\n",
      "Epoch 26/50\n",
      " - 14s - loss: 1.8523 - acc: 0.6362 - val_loss: 2.9968 - val_acc: 0.5514\n",
      "Epoch 27/50\n",
      " - 14s - loss: 1.7510 - acc: 0.6506 - val_loss: 2.9691 - val_acc: 0.5553\n",
      "Epoch 28/50\n",
      " - 14s - loss: 1.6554 - acc: 0.6639 - val_loss: 2.9468 - val_acc: 0.5590\n",
      "Epoch 29/50\n",
      " - 14s - loss: 1.5691 - acc: 0.6766 - val_loss: 2.9334 - val_acc: 0.5636\n",
      "Epoch 30/50\n",
      " - 14s - loss: 1.4865 - acc: 0.6898 - val_loss: 2.9030 - val_acc: 0.5667\n",
      "Epoch 31/50\n",
      " - 14s - loss: 1.4090 - acc: 0.7046 - val_loss: 2.8646 - val_acc: 0.5733\n",
      "Epoch 32/50\n",
      " - 14s - loss: 1.3341 - acc: 0.7172 - val_loss: 2.8443 - val_acc: 0.5770\n",
      "Epoch 33/50\n",
      " - 14s - loss: 1.2666 - acc: 0.7307 - val_loss: 2.8319 - val_acc: 0.5794\n",
      "Epoch 34/50\n",
      " - 14s - loss: 1.2019 - acc: 0.7436 - val_loss: 2.8185 - val_acc: 0.5850\n",
      "Epoch 35/50\n",
      " - 14s - loss: 1.1385 - acc: 0.7583 - val_loss: 2.8193 - val_acc: 0.5823\n",
      "Epoch 36/50\n",
      " - 14s - loss: 1.0769 - acc: 0.7714 - val_loss: 2.7882 - val_acc: 0.5844\n",
      "Epoch 37/50\n",
      " - 14s - loss: 1.0189 - acc: 0.7841 - val_loss: 2.7663 - val_acc: 0.5894\n",
      "Epoch 38/50\n",
      " - 14s - loss: 0.9660 - acc: 0.7961 - val_loss: 2.7631 - val_acc: 0.5895\n",
      "Epoch 39/50\n",
      " - 14s - loss: 0.9199 - acc: 0.8051 - val_loss: 2.7563 - val_acc: 0.5922\n",
      "Epoch 40/50\n",
      " - 14s - loss: 0.8751 - acc: 0.8135 - val_loss: 2.7538 - val_acc: 0.5915\n",
      "Epoch 41/50\n",
      " - 14s - loss: 0.8344 - acc: 0.8210 - val_loss: 2.7522 - val_acc: 0.5929\n",
      "Epoch 42/50\n",
      " - 15s - loss: 0.7937 - acc: 0.8292 - val_loss: 2.7776 - val_acc: 0.5896\n",
      "Epoch 43/50\n",
      " - 16s - loss: 0.7582 - acc: 0.8345 - val_loss: 2.7717 - val_acc: 0.5925\n",
      "Epoch 44/50\n",
      " - 15s - loss: 0.7241 - acc: 0.8408 - val_loss: 2.7463 - val_acc: 0.5950\n",
      "Epoch 45/50\n",
      " - 14s - loss: 0.6922 - acc: 0.8468 - val_loss: 2.7479 - val_acc: 0.5973\n",
      "Epoch 46/50\n",
      " - 14s - loss: 0.6633 - acc: 0.8513 - val_loss: 2.7595 - val_acc: 0.5966\n",
      "Epoch 47/50\n",
      " - 14s - loss: 0.6357 - acc: 0.8560 - val_loss: 2.7608 - val_acc: 0.5965\n",
      "Epoch 48/50\n",
      " - 14s - loss: 0.6096 - acc: 0.8605 - val_loss: 2.7492 - val_acc: 0.5965\n",
      "Epoch 49/50\n",
      " - 15s - loss: 0.5848 - acc: 0.8650 - val_loss: 2.7579 - val_acc: 0.5940\n",
      "Epoch 50/50\n",
      " - 16s - loss: 0.5622 - acc: 0.8695 - val_loss: 2.7517 - val_acc: 0.5963\n",
      "Done\n",
      "Got new file path:  LSTM7.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "def get_new_log_filename(base_dir, network_name):\n",
    "    d = os.path.join(base_dir, network_name);\n",
    "    i = 1;\n",
    "    while True:\n",
    "        potential_name = d + str(i);\n",
    "        if not os.path.exists(potential_name):\n",
    "            print(\"Got new file path: \",potential_name);\n",
    "            return potential_name;\n",
    "        i += 1;\n",
    "\n",
    "def get_new_weight_filename(base_dir, network_name):\n",
    "    d = os.path.join(base_dir, network_name);\n",
    "    i = 1;\n",
    "    while True:\n",
    "        potential_name = d + str(i) + \".hdf5\";\n",
    "        if not os.path.exists(potential_name):\n",
    "            print(\"Got new file path: \",potential_name);\n",
    "            return potential_name;\n",
    "        i += 1;\n",
    "\n",
    "\n",
    "        \n",
    "# Computing inputs for `create_model`\n",
    "input_n = len(eng_word_to_ix)\n",
    "output_n = len(spa_word_to_ix)\n",
    "X_seq_len = len(X[0])\n",
    "Y_seq_len = Y_onehot.shape[1]\n",
    "\n",
    "# Create the model\n",
    "# YOUR CODE HERE\n",
    "model = create_model(input_n, X_seq_len, output_n, Y_seq_len, hidden_dim=128, embedding_dim=128);\n",
    "\n",
    "# Train the model (note that you should feed both the input sentence and a zero array with the correct shape to\n",
    "# the `fit` method).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "LOAD_MODEL = False;\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_weights('LSTM3.hdf5');\n",
    "    print(\"Loaded model\");\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not LOAD_MODEL:\n",
    "    x_input = [X, np.zeros((len(X), 1))];\n",
    "\n",
    "    logdir = get_new_log_filename('logs', 'LSTM');\n",
    "    tb = TensorBoard(log_dir=logdir);\n",
    "    print(\"Training...\");\n",
    "    training_history = model.fit(\n",
    "        batch_size=128,\n",
    "        x=x_input,\n",
    "        y=Y_onehot,\n",
    "        epochs=50,\n",
    "        verbose=2,\n",
    "        validation_split=0.2,\n",
    "        shuffle=False,\n",
    "        callbacks = [tb]\n",
    "    )\n",
    "    print(\"Done\");\n",
    "\n",
    "\n",
    "    file_name = get_new_weight_filename('','LSTM');\n",
    "    model.save_weights(file_name);\n",
    "\n",
    "# If you need, you can use the following line for loading a saved model weights\n",
    "#model.load_weights('seq2seq_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60f558a2b0a63756c5ac087989c5350e",
     "grade": false,
     "grade_id": "cell-4411359602a0cb69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.3 Evaluation <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "You are free to evaluate and compare results of your model(s) in any way you have learned from the deep learning course.  \n",
    "\n",
    "Now motivate your choice of architecture and hyperparameters by answering the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c0c7acfbf28dcfb876fb8259a44e89b",
     "grade": false,
     "grade_id": "cell-5440fa7a5a785784",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** What loss function, metrics and optimizer did you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "423b493471e8b0d54142e1bab890c791",
     "grade": true,
     "grade_id": "cell-369f4073de21366f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** categorical crossentropy: We need to \"classify\" each word, this task can be seen as a multi task classification problem. Metrics = val_acc, acc, val_loss, loss, good for comparing training to validation. Optimizer = Adam(0.001), seemed to perform best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bff435770aa8062226c9b3895a1f6a96",
     "grade": false,
     "grade_id": "cell-b3a5072f283d1dbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** Did you use any Keras callbacks? If so, how did they help you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de7542f0702ad21a0adf46d99618ca84",
     "grade": true,
     "grade_id": "cell-707212a45931de41",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Used Tensorboard to monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "210021597f4c858f6a2509a7397351f4",
     "grade": false,
     "grade_id": "cell-a563bc3066bdc5ad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** How did you evaluate that the model was good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7db7d78e789f3703545dc6ab75f9728",
     "grade": true,
     "grade_id": "cell-2de00c740a7a8a5f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** We weren't able to achieve any better validation accuracy. The model started to overfit instead\n",
    "SInce we only have one LSTM for encoder and decoder it might be difficult achieve higher accuracies. We might need a more complex (deeper) model in order to achieve a better result. Also attention could help in this task, as different words in each sentence are not equally important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d1c1c8df33ecb29a306280ddc562384",
     "grade": false,
     "grade_id": "cell-c70fa415812abf8b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.4 Testing <a class=\"anchor\" id=\"2.2.4\"></a>\n",
    "Test your neural machine translator on a few sentences by running the test case below. The similarity metric is calculated comparing word embeddings.  \n",
    "\n",
    "You can also use the function `translate` to try your own sentences, just remember that every word needs to be in the vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "552a75386c2d6110b5331dada6f9c689",
     "grade": true,
     "grade_id": "cell-e96abd2e0fbbae6d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\" Translate a sentence using `model`. `model` is assumed to be a global variable.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence - a string to translate\n",
    "    \n",
    "    Returns:\n",
    "    the translated sentence as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that each of the words in the input sentence exist in the input dictionary\n",
    "    try:\n",
    "        x = [eng_word_to_ix[word] for word in sentence.split(' ')]\n",
    "    except KeyError as e: \n",
    "        print('{0} doesn\\'t exist in the vocabulary!'.format(e))\n",
    "        \n",
    "    # Pad the input sentence\n",
    "    x = pad_sequences([x], maxlen=SEQ_MAX_LEN, dtype='int32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    x_input = [x, np.zeros((len(x), 1))];\n",
    "    y_pred_words = model.predict(x_input, verbose=0);\n",
    "    pSent = \"\";\n",
    "    for word in y_pred_words[0]:\n",
    "        pSent = pSent + \" \" + spa_ix_to_word[word.argmax()];\n",
    "    \n",
    "    return pSent;\n",
    "    #return y_pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "96ed353059f01b613aae6682d7b936c8",
     "grade": false,
     "grade_id": "cell-6301fda0cbde1746",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"she is a singer .\" \n",
      "Prediction: \" ella ella un idiota .\" \n",
      "Actual \"ella es cantante .\"\n",
      "Similarity: 0.5305793645711944\n",
      "-----------------\n",
      "Input: \"my opinion is irrelevant .\" \n",
      "Prediction: \" mi opinion . irrelevante .\" \n",
      "Actual \"mi opinion es irrelevante .\"\n",
      "Similarity: 0.7289874908676488\n",
      "-----------------\n",
      "Input: \"look out for pickpockets .\" \n",
      "Prediction: \" ojo con los lanzas .\" \n",
      "Actual \"ojo con los lanzas .\"\n",
      "Similarity: 0.9334508921385469\n",
      "-----------------\n",
      "Input: \"this might just work .\" \n",
      "Prediction: \" esto esto mas funcionar .\" \n",
      "Actual \"esto simplemente podria funcionar .\"\n",
      "Similarity: 0.6719615095447313\n",
      "-----------------\n",
      "Input: \"these books are mine .\" \n",
      "Prediction: \" estos libros son mios .\" \n",
      "Actual \"estos libros son mios .\"\n",
      "Similarity: 0.9395346703939538\n",
      "-----------------\n",
      "Input: \"the police are there .\" \n",
      "Prediction: \" la policia esta alli .\" \n",
      "Actual \"la policia esta alli.\"\n",
      "Similarity: 0.9209795482969947\n",
      "-----------------\n",
      "Input: \"the car is very fast .\" \n",
      "Prediction: \" ZERO es muy rapido .\" \n",
      "Actual \"el auto es muy rapido .\"\n",
      "Similarity: 0.8252634944537202\n",
      "-----------------\n",
      "Input: \"thats a stupid idea .\" \n",
      "Prediction: \" es una idea idea .\" \n",
      "Actual \"esa es una idea estupida .\"\n",
      "Similarity: 0.8278089257392145\n",
      "-----------------\n",
      "Input: \"she opens the window .\" \n",
      "Prediction: \" ella gano la ventana .\" \n",
      "Actual \"ella abre la ventana .\"\n",
      "Similarity: 0.793340091357878\n",
      "-----------------\n",
      "Input: \"nothing is happening .\" \n",
      "Prediction: \" no no pasa nada .\" \n",
      "Actual \"no pasa nada .\"\n",
      "Similarity: 0.9102174473914191\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# test case. be sure the variable model has a reference to your trained model\n",
    "test_predictions(translate, eng_word_to_ix, spa_ix_to_word, SEQ_MAX_LEN, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " este es un perro .\n"
     ]
    }
   ],
   "source": [
    "# test for yourself\n",
    "print(translate('this is a dog .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ella ella las cosas .\n"
     ]
    }
   ],
   "source": [
    "print(translate('she love apples .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " el abrio la ventana .\n"
     ]
    }
   ],
   "source": [
    "print(translate('he opened the window .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " esto es bien bien .\n"
     ]
    }
   ],
   "source": [
    "print(translate('this is working well .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ZERO soy buen genial .\n"
     ]
    }
   ],
   "source": [
    "print(translate('i am a great student .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6ca7937953dd9f99a480ffe2a74e426",
     "grade": false,
     "grade_id": "cell-874b0d3a8d72aaa6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Congratulations!\n",
    "You have successfully implemented a recurrent neural network from scratch using only NumPy and also implemented a neural machine translator using Keras!\n",
    "\n",
    "When choosing the architecture for your neural machine translator you are partly restricted from the capabilities of Keras and partly restricted from your available computing power.  \n",
    "\n",
    "**Question:** Give 3 suggestions for different techniques that can be used to improve your neural machine translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0945de0b3c8d25a9bca06c96f6717ce9",
     "grade": true,
     "grade_id": "cell-1b51b4c3f80a3c02",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "1. More LSTMs (deeper network)\n",
    "2. More data (obviously)\n",
    "3. Add attention\n",
    "4. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
